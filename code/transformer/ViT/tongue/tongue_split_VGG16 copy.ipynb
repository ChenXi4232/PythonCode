{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提前进行数据增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 94 image(s) found.\n",
      "Output directory set to ./data/split_dataset/images\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 ground truth image(s) found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=2700x3600 at 0x1D4E83C3350>: 100%|██████████| 1000/1000 [05:05<00:00,  3.27 Samples/s]\n"
     ]
    }
   ],
   "source": [
    "import Augmentor\n",
    "\n",
    "\n",
    "p = Augmentor.Pipeline(\"./data/split_dataset/images\")\n",
    "p.ground_truth(\"./data/split_dataset/annotations\")\n",
    "\n",
    "# 旋转\n",
    "p.rotate(probability=0.5, max_left_rotation=25, max_right_rotation=25)\n",
    "\n",
    "# 翻转\n",
    "p.flip_left_right(probability=0.5)\n",
    "p.flip_top_bottom(probability=0.5)\n",
    "\n",
    "# 缩放\n",
    "# p.zoom_random(probability=0.3, percentage_area=0.9)\n",
    "# p.scale(probability=0.3, scale_factor=1.1)\n",
    "\n",
    "# 小块变形\n",
    "p.random_distortion(probability=0.5, grid_width=10, grid_height=10, magnitude=20)\n",
    "\n",
    "# p.shear(probability=0.5, max_shear_left=10, max_shear_right=10)\n",
    "\n",
    "#随机裁剪\n",
    "p.crop_random(probability=1, percentage_area=0.9)\n",
    "\n",
    "p.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "       247, 248, 249, 250, 251, 252, 253, 254, 255], dtype=uint8)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "img = Image.open(\n",
    "    \"./data/split_dataset_ultra/images/tongue_front_0000351212_2023-10-21-11-21-13.png_9ad366cf-04c0-4577-a94e-1db2d7760ab9.png\")\n",
    "\n",
    "\n",
    "np.unique(np.array(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def jpg_to_png(input_folder, output_folder):\n",
    "    # 确保输出文件夹存在，如果不存在则创建\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # 遍历指定文件夹下的所有文件\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith('.jpg'):\n",
    "            # 构造输入和输出文件的完整路径\n",
    "            input_path = os.path.join(input_folder, filename)\n",
    "            output_path = os.path.join(\n",
    "                output_folder, filename[:-4] + '.png')  # 将文件后缀改为 .png\n",
    "\n",
    "            # 打开 JPG 文件\n",
    "            with Image.open(input_path) as img:\n",
    "                # 读取图像的 Exif 信息，获取方向信息\n",
    "                exif = img._getexif()\n",
    "                orientation = exif.get(0x0112, 1) if exif else 1\n",
    "\n",
    "                # 根据方向信息旋转图像\n",
    "                if orientation == 1:\n",
    "                    pass\n",
    "                elif orientation == 3:\n",
    "                    img = img.rotate(180, expand=True)\n",
    "                elif orientation == 6:\n",
    "                    img = img.rotate(270, expand=True)\n",
    "                elif orientation == 8:\n",
    "                    img = img.rotate(90, expand=True)\n",
    "                # 保存 PNG 文件，并应用原图的方向信息\n",
    "                img.save(output_path, 'PNG')\n",
    "\n",
    "\n",
    "# 指定输入和输出文件夹\n",
    "input_folder = './data/split_dataset/images'\n",
    "output_folder = './data/split_dataset/images/tmp'\n",
    "\n",
    "# 调用函数将 JPG 文件转换为 PNG 文件\n",
    "jpg_to_png(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "def move_and_rename_files(input_folder, output_folder1, output_folder2):\n",
    "    # 如果输出文件夹不存在，则创建\n",
    "    for folder in [output_folder1, output_folder2]:\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "\n",
    "    # 遍历指定文件夹下的所有文件\n",
    "    for filename in os.listdir(input_folder):\n",
    "        # 如果文件名以 \"_groundtruth_(1)_\" 开头\n",
    "        if filename.startswith(\"_groundtruth_(1)_images_\"):\n",
    "            # 构造输入和输出文件的完整路径\n",
    "            input_path = os.path.join(input_folder, filename)\n",
    "            output_path = os.path.join(\n",
    "                output_folder1, filename.replace(\"_groundtruth_(1)_images_\", \"\"))\n",
    "\n",
    "            # 移动并重命名文件\n",
    "            shutil.copy(input_path, output_path)\n",
    "\n",
    "        # 如果文件名以 \"images_original_\" 开头\n",
    "        elif filename.startswith(\"images_original_\"):\n",
    "            # 构造输入和输出文件的完整路径\n",
    "            input_path = os.path.join(input_folder, filename)\n",
    "            output_path = os.path.join(\n",
    "                output_folder2, filename.replace(\"images_original_\", \"\"))\n",
    "\n",
    "            # 移动并重命名文件\n",
    "            shutil.copy(input_path, output_path)\n",
    "\n",
    "\n",
    "# 指定输入和输出文件夹\n",
    "input_folder = './data/split_dataset/images/output'\n",
    "output_folder1 = './data/split_dataset_ultra/annotations'\n",
    "output_folder2 = './data/split_dataset_ultra/images'\n",
    "\n",
    "# 调用函数移动和重命名文件\n",
    "move_and_rename_files(input_folder, output_folder1, output_folder2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n",
      "['_background_', 'Tg']\n",
      "torch.Size([3, 3000, 4000])\n",
      "torch.Size([1, 4000, 3000])\n",
      "tensor([0.0000, 0.0039])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "\n",
    "class TongueDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_dir = os.path.join(root_dir, 'images')\n",
    "        self.mask_dir = os.path.join(root_dir, 'annotations')\n",
    "        self.class_names = self.load_class_names()\n",
    "\n",
    "    def load_class_names(self):\n",
    "        class_names_file = os.path.join(self.root_dir, 'class_names.txt')\n",
    "        with open(class_names_file, 'r') as f:\n",
    "            class_names = f.readlines()\n",
    "        class_names = [name.strip() for name in class_names]\n",
    "        return class_names\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.image_dir))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.listdir(self.image_dir)[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        mask_path = os.path.join(\n",
    "            self.mask_dir, img_name.split('.')[0] + '.png')\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        mask = Image.open(mask_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = TongueDataset(root_dir='./data/split_dataset', transform=data_transform)\n",
    "\n",
    "print(len(dataset))\n",
    "print(dataset.class_names)\n",
    "print(dataset[0][0].shape)\n",
    "print(dataset[0][1].shape)\n",
    "print(dataset[0][1].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3000, 4000])\n",
      "均值：tensor([0.5687, 0.5235, 0.4903])\n",
      "标准差：tensor([0.2281, 0.2190, 0.2076])\n"
     ]
    }
   ],
   "source": [
    "# 统计数据集中图像的均值和标准差\n",
    "dataloader = DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
    "\n",
    "all_images = []\n",
    "for images, _ in dataloader:\n",
    "    all_images = images\n",
    "\n",
    "print(all_images[0].shape)\n",
    "\n",
    "# 计算数据集的均值和标准差\n",
    "mean = torch.mean(all_images, dim=(0, 2, 3))\n",
    "std = torch.std(all_images, dim=(0, 2, 3))\n",
    "\n",
    "print(f\"均值：{mean}\")\n",
    "print(f\"标准差：{std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "import Augmentor\n",
    "\n",
    "\n",
    "\n",
    "not_pos_transform = transforms.Compose([\n",
    "\n",
    "    transforms.RandomApply(transforms.ColorJitter(brightness=0.2, contrast=0.2,\n",
    "                           saturation=0.2, hue=0.1)),\n",
    "    transforms.RandomApply(transforms.GaussianBlur(3, sigma=(0.1, 2.0)), 0.5),\n",
    "\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "    transforms.Normalize(mean, std),\n",
    "\n",
    "])\n",
    "\n",
    "pos_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomCrop((112, 112)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(45),\n",
    "])\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "\n",
    "    transforms.Resize((224, 224)),\n",
    "\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "    transforms.Normalize(mean, std),\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "# 划分数据集\n",
    "\n",
    "train_size = int(0.75 * len(dataset))\n",
    "\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "val_dataloader.dataset.transform = transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedDataset(Dataset):\n",
    "    def __init__(self, dataset, augmented_dataset_size):\n",
    "        self.dataset = dataset\n",
    "        self.augmented_dataset_size = augmented_dataset_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) * self.augmented_dataset_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        dataset_idx = idx % len(self.dataset)\n",
    "        return self.dataset[dataset_idx]\n",
    "\n",
    "\n",
    "# 扩充数据集\n",
    "augmented_dataset_size = 8\n",
    "train_dataset = AugmentedDataset(train_dataset, augmented_dataset_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "train_dataloader.dataset.transform = data_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# 定义基于VGG16的FCN网络\n",
    "class VGG16_FCN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(VGG16_FCN, self).__init__()\n",
    "        # 加载预训练的VGG16模型\n",
    "        vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "        # 取出VGG16的前面部分（去掉全连接层）\n",
    "        self.features = vgg16.features\n",
    "\n",
    "        # 用1x1卷积替换VGG16的全连接层\n",
    "        self.conv6 = nn.Conv2d(512, 4096, kernel_size=1)\n",
    "        self.relu6 = nn.ReLU(inplace=True)\n",
    "        self.dropout6 = nn.Dropout2d()\n",
    "        self.conv7 = nn.Conv2d(4096, 4096, kernel_size=1)\n",
    "        self.relu7 = nn.ReLU(inplace=True)\n",
    "        self.dropout7 = nn.Dropout2d()\n",
    "\n",
    "        # 最后的卷积层用于生成分割结果\n",
    "        self.score_fr = nn.Conv2d(4096, num_classes, kernel_size=1)\n",
    "        self.upscore = nn.ConvTranspose2d(\n",
    "            num_classes, num_classes, kernel_size=32, stride=32, bias=False)\n",
    "\n",
    "        # 初始化上采样层权重\n",
    "        self.upscore.weight.data.fill_(0)\n",
    "        self.upscore.weight.data[:, :, 16, 16] = 1  # 双线性插值\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 前向传播\n",
    "        x = self.features(x)\n",
    "        x = self.relu6(self.conv6(x))\n",
    "        x = self.dropout6(x)\n",
    "        x = self.relu7(self.conv7(x))\n",
    "        x = self.dropout7(x)\n",
    "        x = self.score_fr(x)\n",
    "        x = self.upscore(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SegmentationLoss(nn.Module):\n",
    "    def __init__(self, initial_weight_bce=0.5, initial_weight_connectivity=0.5, initial_weight_smoothness=0.5):\n",
    "        super(SegmentationLoss, self).__init__()\n",
    "        self.weight_bce = initial_weight_bce\n",
    "        self.weight_connectivity = nn.Parameter(\n",
    "            torch.tensor(initial_weight_connectivity))\n",
    "        self.weight_smoothness = nn.Parameter(\n",
    "            torch.tensor(initial_weight_smoothness))\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        self.smooth = 1e-6\n",
    "\n",
    "    def forward(self, logits, masks):\n",
    "        # 计算 BCE Loss\n",
    "        bce_loss = self.bce_loss(logits, masks)\n",
    "\n",
    "        # 计算 Dice Loss\n",
    "        probs = torch.sigmoid(logits)\n",
    "        intersection = torch.sum(probs * masks)\n",
    "        dice_loss = 1 - (2. * intersection + self.smooth) / \\\n",
    "            (torch.sum(probs) + torch.sum(masks) + self.smooth)\n",
    "\n",
    "        # 计算连通性损失\n",
    "        connectivity_loss = self.connectivity_loss(masks)\n",
    "\n",
    "        # 计算平滑性损失\n",
    "        smoothness_loss = self.smoothness_loss(masks)\n",
    "\n",
    "        # 加权结合损失\n",
    "        combined_loss = self.weight_bce * bce_loss + \\\n",
    "            (1 - self.weight_bce) * dice_loss + \\\n",
    "            self.weight_connectivity * connectivity_loss + \\\n",
    "            self.weight_smoothness * smoothness_loss\n",
    "\n",
    "        return combined_loss\n",
    "\n",
    "    # 连通性损失函数\n",
    "    def connectivity_loss(self, mask):\n",
    "        connected_components = torch.unique(mask)\n",
    "        count = 0\n",
    "        for value in connected_components:\n",
    "            binary_mask = (mask == value).float()\n",
    "            _, labels = binary_mask.view(1, -1).unique(dim=1, return_inverse=True)\n",
    "            count += labels.max().item() + 1\n",
    "        loss = count - 1\n",
    "        return loss\n",
    "\n",
    "    # 平滑性损失函数\n",
    "    def smoothness_loss(self, mask):\n",
    "        sobel_x = torch.abs(F.conv2d(mask, torch.cuda.FloatTensor(\n",
    "            [[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]).unsqueeze(0).unsqueeze(0)))\n",
    "        sobel_y = torch.abs(F.conv2d(mask, torch.cuda.FloatTensor(\n",
    "            [[-1, -2, -1], [0, 0, 0], [1, 2, 1]]).unsqueeze(0).unsqueeze(0)))\n",
    "        edge_curvature = torch.mean(sobel_x + sobel_y)\n",
    "        loss = edge_curvature\n",
    "        return loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in tqdm(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch +\n",
    "              1, num_epochs, epoch_loss))\n",
    "\n",
    "\n",
    "def get_segmentation_mask(model, image_path):\n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[\n",
    "                             0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    image = transform(image).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "    output = torch.argmax(output, dim=1)\n",
    "    mask = output.squeeze().cpu().numpy()\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ChenXi\\.conda\\envs\\2.2.1_11.8\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ChenXi\\.conda\\envs\\2.2.1_11.8\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "100%|██████████| 280/280 [01:23<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.0182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 280/280 [01:27<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 2.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 13/280 [00:04<01:23,  3.18it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Get segmentation mask for a sample image\u001b[39;00m\n\u001b[0;32m     14\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_image.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with your image path\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[61], line 8\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m      7\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m----> 8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ChenXi\\.conda\\envs\\2.2.1_11.8\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\ChenXi\\.conda\\envs\\2.2.1_11.8\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\ChenXi\\.conda\\envs\\2.2.1_11.8\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\ChenXi\\.conda\\envs\\2.2.1_11.8\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\ChenXi\\.conda\\envs\\2.2.1_11.8\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[30], line 11\u001b[0m, in \u001b[0;36mAugmentedDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     10\u001b[0m     dataset_idx \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_idx\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ChenXi\\.conda\\envs\\2.2.1_11.8\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:391\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[1;32m--> 391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[1;32mIn[44], line 39\u001b[0m, in \u001b[0;36mTongueDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     35\u001b[0m img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_dir, img_name)\n\u001b[0;32m     36\u001b[0m mask_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_dir, img_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 39\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m mask \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(mask_path)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n",
      "File \u001b[1;32mc:\\Users\\ChenXi\\.conda\\envs\\2.2.1_11.8\\Lib\\site-packages\\PIL\\Image.py:922\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[0;32m    876\u001b[0m ):\n\u001b[0;32m    877\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 922\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    924\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[0;32m    925\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    926\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ChenXi\\.conda\\envs\\2.2.1_11.8\\Lib\\site-packages\\PIL\\ImageFile.py:291\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    290\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 291\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    num_classes = 1  # Background and tongue\n",
    "    model = VGG16_FCN(num_classes)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = SegmentationLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    # Train the model\n",
    "    train_model(model, train_dataloader, criterion, optimizer)\n",
    "\n",
    "    # Get segmentation mask for a sample image\n",
    "    image_path = 'sample_image.jpg'  # Replace with your image path\n",
    "    mask = get_segmentation_mask(model, image_path)\n",
    "\n",
    "    # Create the final mask where tongue region is white and background is black\n",
    "    final_mask = np.zeros_like(mask)\n",
    "    final_mask[mask == 1] = 255\n",
    "\n",
    "    # Save or visualize the final mask as needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2.2.1_11.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
