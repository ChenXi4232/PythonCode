{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352\n",
      "['an_hong', 'bai_tai', 'bao_tai', 'bo_luo', 'chi_hen', 'dan_bai', 'dan_hong', 'dian_ci', 'duan_suo', 'fu_tai', 'hou_tai', 'hua_tai', 'huang_tai', 'hui_hei', 'jiang_she', 'lie_wen', 'ni_tai', 'pang_da', 'qing_zi', 'shao_tai', 'she_jian_hong', 'wai_xie', 'wei_ruan', 'wu_tai']\n",
      "{'an_hong': 0, 'bai_tai': 1, 'bao_tai': 2, 'bo_luo': 3, 'chi_hen': 4, 'dan_bai': 5, 'dan_hong': 6, 'dian_ci': 7, 'duan_suo': 8, 'fu_tai': 9, 'hou_tai': 10, 'hua_tai': 11, 'huang_tai': 12, 'hui_hei': 13, 'jiang_she': 14, 'lie_wen': 15, 'ni_tai': 16, 'pang_da': 17, 'qing_zi': 18, 'shao_tai': 19, 'she_jian_hong': 20, 'wai_xie': 21, 'wei_ruan': 22, 'wu_tai': 23}\n",
      "[('./data/dataset\\\\an_hong\\\\tongue_front_300110899004_2023-11-08-15-36-06.jpg', 0), ('./data/dataset\\\\an_hong\\\\tongue_front_300136928001_2023-11-07-10-46-40.jpg', 0), ('./data/dataset\\\\bai_tai\\\\tongue_front_0000351212_2023-10-21-11-21-13.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_200011111001_2023-10-24-09-58-09.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_200011111002_2023-10-24-10-23-23.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_20231106001_2023-11-06-16-47-26.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_202311070001_2023-11-07-09-42-50.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_202311070002_2023-11-07-11-29-19.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300021285011_2023-10-19-10-54-58.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300033407009_2023-10-21-09-19-22.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300034041006_2023-11-08-15-03-31.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300050923002_2023-11-04-15-27-09.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300056160005_2023-10-26-16-12-05.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300064508006_2023-10-23-17-36-28.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300073033004_2023-10-26-11-11-05.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300085183002_2023-10-24-11-34-46.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300086582001_2023-10-21-15-39-18.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300093805006_2023-10-26-14-46-17.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300098764015_2023-10-21-14-40-56.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300099120001_2023-11-07-16-42-33.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300103974003_2023-10-21-10-26-48.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300110899004_2023-11-08-15-36-06.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300114610004_2023-10-23-10-33-50.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300119198005_2023-10-26-15-07-02.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300122419002_2023-11-06-14-22-38.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300124851007_2023-10-23-10-11-38.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300124986_2023-10-19-15-15-42.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300126157003 _2023-11-07-11-07-19.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300128281002_2023-11-07-10-16-41.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300129208004_2023-10-24-14-49-10.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300130785002_2023-10-21-10-00-00.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300132291003_2023-10-21-16-50-10.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300132464003_2023-10-20-09-55-35.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300134481_2023-11-03-17-06-07.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300135824002_2023-10-23-11-03-41.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300136416001_2023-10-21-16-04-21.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300136486001_2023-10-26-10-30-40.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300136682001_2023-10-24-15-28-45.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300136809001_2023-10-26-11-40-20.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300136852001_2023-10-21-09-41-22.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300136861001_2023-10-24-16-01-39.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300136899001 _2023-10-25-15-35-47.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300136907_2023-10-14-11-00-02.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300136928001_2023-11-07-10-46-40.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300136932_2023-11-03-15-17-18.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300137097001_2023-11-06-14-53-18.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300137144001_2023-10-21-10-43-07.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300137163001_2023-10-25-16-51-47.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300137245001_2023-10-26-15-27-53.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300137334001_2023-10-21-15-06-31.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300137471_2023-10-24-17-00-40.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300137605001_2023-10-25-17-09-26.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300137710001_2023-11-04-14-52-37.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300137767001_2023-11-07-16-20-58.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300137871001_2023-11-06-15-10-33.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300137880001_2023-11-06-15-33-32.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300138026001_2023-11-06-16-06-05.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300138047001_2023-11-04-16-34-30.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300138095001_2023-11-06-13-59-35.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300138133001_2023-11-06-16-30-04.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300138197001_2023-11-07-15-17-08.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_300138204_2023-11-07-15-36-45.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_400110260001_2023-10-26-15-49-59.jpg', 1), ('./data/dataset\\\\bai_tai\\\\tongue_front_400110260002_2023-10-26-17-05-17.jpg', 1), ('./data/dataset\\\\bao_tai\\\\tongue_front_0000351212_2023-10-21-11-21-13.jpg', 2), ('./data/dataset\\\\bao_tai\\\\tongue_front_202311070001_2023-11-07-09-42-50.jpg', 2), ('./data/dataset\\\\bao_tai\\\\tongue_front_300063112015_2023-10-20-09-19-28.jpg', 2), ('./data/dataset\\\\bao_tai\\\\tongue_front_300064508006_2023-10-23-17-36-28.jpg', 2), ('./data/dataset\\\\bao_tai\\\\tongue_front_300085183002_2023-10-24-11-34-46.jpg', 2), ('./data/dataset\\\\bao_tai\\\\tongue_front_300086582001_2023-10-21-15-39-18.jpg', 2), ('./data/dataset\\\\bao_tai\\\\tongue_front_300093363004_2023-10-23-12-01-55.jpg', 2), ('./data/dataset\\\\bao_tai\\\\tongue_front_300098764015_2023-10-21-14-40-56.jpg', 2), ('./data/dataset\\\\bao_tai\\\\tongue_front_300110899004_2023-11-08-15-36-06.jpg', 2), ('./data/dataset\\\\bao_tai\\\\tongue_front_300114610004_2023-10-23-10-33-50.jpg', 2), ('./data/dataset\\\\bao_tai\\\\tongue_front_300124851007_2023-10-23-10-11-38.jpg', 2), ('./data/dataset\\\\bao_tai\\\\tongue_front_300124986_2023-10-19-15-15-42.jpg', 2), ('./data/dataset\\\\bao_tai\\\\tongue_front_300125023004_2023-11-06-13-26-15.jpg', 2), ('./data/dataset\\\\bao_tai\\\\tongue_front_300129208004_2023-10-24-14-49-10.jpg', 2), ('./data/dataset\\\\bao_tai\\\\tongue_front_300132291003_2023-10-21-16-50-10.jpg', 2), ('./data/dataset\\\\bao_tai\\\\tongue_front_300132464003_2023-10-20-09-55-35.jpg', 2), ('./data/dataset\\\\bao_tai\\\\tongue_front_300136682001_2023-10-24-15-28-45.jpg', 2), ('./data/dataset\\\\bao_tai\\\\tongue_front_300137076001_2023-10-23-15-09-38.jpg', 2), ('./data/dataset\\\\bao_tai\\\\tongue_front_300137097001_2023-11-06-14-53-18.jpg', 2), ('./data/dataset\\\\bao_tai\\\\tongue_front_300137471_2023-10-24-17-00-40.jpg', 2), ('./data/dataset\\\\bao_tai\\\\tongue_front_300137495001_2023-11-03-16-04-27.jpg', 2), ('./data/dataset\\\\bao_tai\\\\tongue_front_300137605001_2023-10-25-17-09-26.jpg', 2), ('./data/dataset\\\\bao_tai\\\\tongue_front_300138197001_2023-11-07-15-17-08.jpg', 2), ('./data/dataset\\\\bo_luo\\\\tongue_front_300126157003 _2023-11-07-11-07-19.jpg', 3), ('./data/dataset\\\\bo_luo\\\\tongue_front_300136899001 _2023-10-25-15-35-47.jpg', 3), ('./data/dataset\\\\chi_hen\\\\tongue_front_0000351212_2023-10-21-11-21-13.jpg', 4), ('./data/dataset\\\\chi_hen\\\\tongue_front_197010022143_2023-10-14-10-39-30.jpg', 4), ('./data/dataset\\\\chi_hen\\\\tongue_front_20231106001_2023-11-06-16-47-26.jpg', 4), ('./data/dataset\\\\chi_hen\\\\tongue_front_202311070002_2023-11-07-11-29-19.jpg', 4), ('./data/dataset\\\\chi_hen\\\\tongue_front_300021285011_2023-10-19-10-54-58.jpg', 4), ('./data/dataset\\\\chi_hen\\\\tongue_front_300083125008_2023-10-21-10-58-43.jpg', 4), ('./data/dataset\\\\chi_hen\\\\tongue_front_300085183002_2023-10-24-11-34-46.jpg', 4), ('./data/dataset\\\\chi_hen\\\\tongue_front_300093363004_2023-10-23-12-01-55.jpg', 4), ('./data/dataset\\\\chi_hen\\\\tongue_front_300122419002_2023-11-06-14-22-38.jpg', 4), ('./data/dataset\\\\chi_hen\\\\tongue_front_300124785005_2023-10-21-16-21-35.jpg', 4), ('./data/dataset\\\\chi_hen\\\\tongue_front_300124851007_2023-10-23-10-11-38.jpg', 4), ('./data/dataset\\\\chi_hen\\\\tongue_front_300124986_2023-10-19-15-15-42.jpg', 4), ('./data/dataset\\\\chi_hen\\\\tongue_front_300133303002_2023-10-25-15-58-08.jpg', 4), ('./data/dataset\\\\chi_hen\\\\tongue_front_300134481_2023-11-03-17-06-07.jpg', 4), ('./data/dataset\\\\chi_hen\\\\tongue_front_300136440001_2023-10-20-10-19-20.jpg', 4), ('./data/dataset\\\\chi_hen\\\\tongue_front_300136486001_2023-10-26-10-30-40.jpg', 4), ('./data/dataset\\\\chi_hen\\\\tongue_front_300136852001_2023-10-21-09-41-22.jpg', 4), ('./data/dataset\\\\chi_hen\\\\tongue_front_300136941001_2023-10-26-10-41-16.jpg', 4), ('./data/dataset\\\\chi_hen\\\\tongue_front_300137097001_2023-11-06-14-53-18.jpg', 4), ('./data/dataset\\\\chi_hen\\\\tongue_front_300137245001_2023-10-26-15-27-53.jpg', 4), ('./data/dataset\\\\chi_hen\\\\tongue_front_300137334001_2023-10-21-15-06-31.jpg', 4), ('./data/dataset\\\\chi_hen\\\\tongue_front_300137495001_2023-11-03-16-04-27.jpg', 4), ('./data/dataset\\\\chi_hen\\\\tongue_front_300137694001_2023-11-07-09-15-29.jpg', 4), ('./data/dataset\\\\chi_hen\\\\tongue_front_300137767001_2023-11-07-16-20-58.jpg', 4), ('./data/dataset\\\\chi_hen\\\\tongue_front_300137880001_2023-11-06-15-33-32.jpg', 4), ('./data/dataset\\\\chi_hen\\\\tongue_front_300138047001_2023-11-04-16-34-30.jpg', 4), ('./data/dataset\\\\chi_hen\\\\tongue_front_400110260001_2023-10-26-15-49-59.jpg', 4), ('./data/dataset\\\\chi_hen\\\\tongue_front_400110260002_2023-10-26-17-05-17.jpg', 4), ('./data/dataset\\\\dan_bai\\\\tongue_front_200012345001_2023-10-24-09-27-03.jpg', 5), ('./data/dataset\\\\dan_bai\\\\tongue_front_300033407009_2023-10-21-09-19-22.jpg', 5), ('./data/dataset\\\\dan_bai\\\\tongue_front_300056160005_2023-10-26-16-12-05.jpg', 5), ('./data/dataset\\\\dan_bai\\\\tongue_front_300073033004_2023-10-26-11-11-05.jpg', 5), ('./data/dataset\\\\dan_bai\\\\tongue_front_300093805006_2023-10-26-14-46-17.jpg', 5), ('./data/dataset\\\\dan_bai\\\\tongue_front_300103974003_2023-10-21-10-26-48.jpg', 5), ('./data/dataset\\\\dan_bai\\\\tongue_front_300124785005_2023-10-21-16-21-35.jpg', 5), ('./data/dataset\\\\dan_bai\\\\tongue_front_300128281002_2023-11-07-10-16-41.jpg', 5), ('./data/dataset\\\\dan_bai\\\\tongue_front_300136416001_2023-10-21-16-04-21.jpg', 5), ('./data/dataset\\\\dan_bai\\\\tongue_front_300136630001_2023-10-25-16-19-59.jpg', 5), ('./data/dataset\\\\dan_bai\\\\tongue_front_300136932_2023-11-03-15-17-18.jpg', 5), ('./data/dataset\\\\dan_bai\\\\tongue_front_300137694001_2023-11-07-09-15-29.jpg', 5), ('./data/dataset\\\\dan_bai\\\\tongue_front_300137767001_2023-11-07-16-20-58.jpg', 5), ('./data/dataset\\\\dan_bai\\\\tongue_front_300138026001_2023-11-06-16-06-05.jpg', 5), ('./data/dataset\\\\dan_bai\\\\tongue_front_300138095001_2023-11-06-13-59-35.jpg', 5), ('./data/dataset\\\\dan_hong\\\\tongue_front_0000351212_2023-10-21-11-21-13.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_197010022143_2023-10-14-10-39-30.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_200011111002_2023-10-24-10-23-23.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_20231106001_2023-11-06-16-47-26.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_202311070001_2023-11-07-09-42-50.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_202311070002_2023-11-07-11-29-19.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300022230004_2023-10-19-11-26-07.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300034041006_2023-11-08-15-03-31.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300050923002_2023-11-04-15-27-09.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300063112015_2023-10-20-09-19-28.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300066394010_2023-10-20-11-05-01.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300083125008_2023-10-21-10-58-43.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300085183002_2023-10-24-11-34-46.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300086582001_2023-10-21-15-39-18.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300093363004_2023-10-23-12-01-55.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300098764015_2023-10-21-14-40-56.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300099120001_2023-11-07-16-42-33.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300114610004_2023-10-23-10-33-50.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300119198005_2023-10-26-15-07-02.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300122419002_2023-11-06-14-22-38.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300124851007_2023-10-23-10-11-38.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300124986_2023-10-19-15-15-42.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300125023004_2023-11-06-13-26-15.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300129208004_2023-10-24-14-49-10.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300130785002_2023-10-21-10-00-00.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300132291003_2023-10-21-16-50-10.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300132464003_2023-10-20-09-55-35.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300133303002_2023-10-25-15-58-08.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300134481_2023-11-03-17-06-07.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300136440001_2023-10-20-10-19-20.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300136486001_2023-10-26-10-30-40.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300136682001_2023-10-24-15-28-45.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300136852001_2023-10-21-09-41-22.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300136861001_2023-10-24-16-01-39.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300136899001 _2023-10-25-15-35-47.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300136907_2023-10-14-11-00-02.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300136941001_2023-10-26-10-41-16.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300137076001_2023-10-23-15-09-38.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300137097001_2023-11-06-14-53-18.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300137245001_2023-10-26-15-27-53.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300137272_2023-11-03-16-45-48.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300137325001_2023-10-23-16-21-54.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300137334001_2023-10-21-15-06-31.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300137468001_2023-10-23-14-47-04.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300137495001_2023-11-03-16-04-27.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300137605001_2023-10-25-17-09-26.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300137710001_2023-11-04-14-52-37.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300137764001_2023-11-04-16-04-15.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300137871001_2023-11-06-15-10-33.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300137880001_2023-11-06-15-33-32.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300138047001_2023-11-04-16-34-30.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_300138197001_2023-11-07-15-17-08.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_400110260001_2023-10-26-15-49-59.jpg', 6), ('./data/dataset\\\\dan_hong\\\\tongue_front_400110260002_2023-10-26-17-05-17.jpg', 6), ('./data/dataset\\\\dian_ci\\\\tongue_front_300137144001_2023-10-21-10-43-07.jpg', 7), ('./data/dataset\\\\duan_suo\\\\tongue_front_300050923002_2023-11-04-15-27-09.jpg', 8), ('./data/dataset\\\\duan_suo\\\\tongue_front_300093805006_2023-10-26-14-46-17.jpg', 8), ('./data/dataset\\\\duan_suo\\\\tongue_front_300136428001_2023-10-24-10-46-33.jpg', 8), ('./data/dataset\\\\duan_suo\\\\tongue_front_300136440001_2023-10-20-10-19-20.jpg', 8), ('./data/dataset\\\\duan_suo\\\\tongue_front_300136861001_2023-10-24-16-01-39.jpg', 8), ('./data/dataset\\\\duan_suo\\\\tongue_front_300136899001 _2023-10-25-15-35-47.jpg', 8), ('./data/dataset\\\\fu_tai\\\\tongue_front_300099120001_2023-11-07-16-42-33.jpg', 9), ('./data/dataset\\\\fu_tai\\\\tongue_front_300135893_2023-10-19-15-54-14.jpg', 9), ('./data/dataset\\\\fu_tai\\\\tongue_front_300137764001_2023-11-04-16-04-15.jpg', 9), ('./data/dataset\\\\fu_tai\\\\tongue_front_300138204_2023-11-07-15-36-45.jpg', 9), ('./data/dataset\\\\fu_tai\\\\tongue_front_400110260002_2023-10-26-17-05-17.jpg', 9), ('./data/dataset\\\\hou_tai\\\\tongue_front_300136630001_2023-10-25-16-19-59.jpg', 10), ('./data/dataset\\\\hua_tai\\\\tongue_front_300061025010_2023-11-06-17-42-49.jpg', 11), ('./data/dataset\\\\hua_tai\\\\tongue_front_300098905003_2023-11-06-17-20-53.jpg', 11), ('./data/dataset\\\\huang_tai\\\\tongue_front_197010022143_2023-10-14-10-39-30.jpg', 12), ('./data/dataset\\\\huang_tai\\\\tongue_front_200012345001_2023-10-24-09-27-03.jpg', 12), ('./data/dataset\\\\huang_tai\\\\tongue_front_300022230004_2023-10-19-11-26-07.jpg', 12), ('./data/dataset\\\\huang_tai\\\\tongue_front_300063112015_2023-10-20-09-19-28.jpg', 12), ('./data/dataset\\\\huang_tai\\\\tongue_front_300066394010_2023-10-20-11-05-01.jpg', 12), ('./data/dataset\\\\huang_tai\\\\tongue_front_300083125008_2023-10-21-10-58-43.jpg', 12), ('./data/dataset\\\\huang_tai\\\\tongue_front_300093363004_2023-10-23-12-01-55.jpg', 12), ('./data/dataset\\\\huang_tai\\\\tongue_front_300098905003_2023-11-06-17-20-53.jpg', 12), ('./data/dataset\\\\huang_tai\\\\tongue_front_300116371003_2023-11-04-17-01-24.jpg', 12), ('./data/dataset\\\\huang_tai\\\\tongue_front_300124543006_2023-10-23-11-26-17.jpg', 12), ('./data/dataset\\\\huang_tai\\\\tongue_front_300124785005_2023-10-21-16-21-35.jpg', 12), ('./data/dataset\\\\huang_tai\\\\tongue_front_300125023004_2023-11-06-13-26-15.jpg', 12), ('./data/dataset\\\\huang_tai\\\\tongue_front_300133303002_2023-10-25-15-58-08.jpg', 12), ('./data/dataset\\\\huang_tai\\\\tongue_front_300135893_2023-10-19-15-54-14.jpg', 12), ('./data/dataset\\\\huang_tai\\\\tongue_front_300136440001_2023-10-20-10-19-20.jpg', 12), ('./data/dataset\\\\huang_tai\\\\tongue_front_300136941001_2023-10-26-10-41-16.jpg', 12), ('./data/dataset\\\\huang_tai\\\\tongue_front_300137076001_2023-10-23-15-09-38.jpg', 12), ('./data/dataset\\\\huang_tai\\\\tongue_front_300137151001_2023-10-24-16-31-03.jpg', 12), ('./data/dataset\\\\huang_tai\\\\tongue_front_300137272_2023-11-03-16-45-48.jpg', 12), ('./data/dataset\\\\huang_tai\\\\tongue_front_300137325001_2023-10-23-16-21-54.jpg', 12), ('./data/dataset\\\\huang_tai\\\\tongue_front_300137388001_2023-10-26-11-26-40.jpg', 12), ('./data/dataset\\\\huang_tai\\\\tongue_front_300137468001_2023-10-23-14-47-04.jpg', 12), ('./data/dataset\\\\huang_tai\\\\tongue_front_300137495001_2023-11-03-16-04-27.jpg', 12), ('./data/dataset\\\\huang_tai\\\\tongue_front_300137669001_2023-11-07-14-48-44.jpg', 12), ('./data/dataset\\\\huang_tai\\\\tongue_front_300137694001_2023-11-07-09-15-29.jpg', 12), ('./data/dataset\\\\huang_tai\\\\tongue_front_300137764001_2023-11-04-16-04-15.jpg', 12), ('./data/dataset\\\\huang_tai\\\\tongue_front_300138007001_2023-11-07-15-58-20.jpg', 12), ('./data/dataset\\\\hui_hei\\\\tongue_front_300061025010_2023-11-06-17-42-49.jpg', 13), ('./data/dataset\\\\hui_hei\\\\tongue_front_300136630001_2023-10-25-16-19-59.jpg', 13), ('./data/dataset\\\\jiang_she\\\\tongue_front_300021285011_2023-10-19-10-54-58.jpg', 14), ('./data/dataset\\\\jiang_she\\\\tongue_front_300131748002_2023-11-08-16-01-59.jpg', 14), ('./data/dataset\\\\jiang_she\\\\tongue_front_300136428001_2023-10-24-10-46-33.jpg', 14), ('./data/dataset\\\\jiang_she\\\\tongue_front_300137471_2023-10-24-17-00-40.jpg', 14), ('./data/dataset\\\\jiang_she\\\\tongue_front_300138133001_2023-11-06-16-30-04.jpg', 14), ('./data/dataset\\\\jiang_she\\\\tongue_front_300138204_2023-11-07-15-36-45.jpg', 14), ('./data/dataset\\\\lie_wen\\\\tongue_front_200012345001_2023-10-24-09-27-03.jpg', 15), ('./data/dataset\\\\lie_wen\\\\tongue_front_300064508006_2023-10-23-17-36-28.jpg', 15), ('./data/dataset\\\\lie_wen\\\\tongue_front_300086582001_2023-10-21-15-39-18.jpg', 15), ('./data/dataset\\\\lie_wen\\\\tongue_front_300110899004_2023-11-08-15-36-06.jpg', 15), ('./data/dataset\\\\lie_wen\\\\tongue_front_300124851007_2023-10-23-10-11-38.jpg', 15), ('./data/dataset\\\\lie_wen\\\\tongue_front_300131748002_2023-11-08-16-01-59.jpg', 15), ('./data/dataset\\\\lie_wen\\\\tongue_front_300132464003_2023-10-20-09-55-35.jpg', 15), ('./data/dataset\\\\lie_wen\\\\tongue_front_300136486001_2023-10-26-10-30-40.jpg', 15), ('./data/dataset\\\\lie_wen\\\\tongue_front_300136630001_2023-10-25-16-19-59.jpg', 15), ('./data/dataset\\\\lie_wen\\\\tongue_front_300136907_2023-10-14-11-00-02.jpg', 15), ('./data/dataset\\\\lie_wen\\\\tongue_front_300137151001_2023-10-24-16-31-03.jpg', 15), ('./data/dataset\\\\lie_wen\\\\tongue_front_300137163001_2023-10-25-16-51-47.jpg', 15), ('./data/dataset\\\\lie_wen\\\\tongue_front_300137388001_2023-10-26-11-26-40.jpg', 15), ('./data/dataset\\\\lie_wen\\\\tongue_front_300137471_2023-10-24-17-00-40.jpg', 15), ('./data/dataset\\\\lie_wen\\\\tongue_front_300137669001_2023-11-07-14-48-44.jpg', 15), ('./data/dataset\\\\lie_wen\\\\tongue_front_300137764001_2023-11-04-16-04-15.jpg', 15), ('./data/dataset\\\\lie_wen\\\\tongue_front_300137871001_2023-11-06-15-10-33.jpg', 15), ('./data/dataset\\\\lie_wen\\\\tongue_front_300138007001_2023-11-07-15-58-20.jpg', 15), ('./data/dataset\\\\ni_tai\\\\tongue_front_197010022143_2023-10-14-10-39-30.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_200011111001_2023-10-24-09-58-09.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_200011111002_2023-10-24-10-23-23.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_200012345001_2023-10-24-09-27-03.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_20231106001_2023-11-06-16-47-26.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_202311070002_2023-11-07-11-29-19.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300022230004_2023-10-19-11-26-07.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300033407009_2023-10-21-09-19-22.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300034041006_2023-11-08-15-03-31.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300050923002_2023-11-04-15-27-09.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300056160005_2023-10-26-16-12-05.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300066394010_2023-10-20-11-05-01.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300073033004_2023-10-26-11-11-05.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300083125008_2023-10-21-10-58-43.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300093805006_2023-10-26-14-46-17.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300103974003_2023-10-21-10-26-48.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300116371003_2023-11-04-17-01-24.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300119198005_2023-10-26-15-07-02.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300122419002_2023-11-06-14-22-38.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300124543006_2023-10-23-11-26-17.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300124785005_2023-10-21-16-21-35.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300128281002_2023-11-07-10-16-41.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300130785002_2023-10-21-10-00-00.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300133303002_2023-10-25-15-58-08.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300134481_2023-11-03-17-06-07.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300135824002_2023-10-23-11-03-41.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300136416001_2023-10-21-16-04-21.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300136440001_2023-10-20-10-19-20.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300136486001_2023-10-26-10-30-40.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300136809001_2023-10-26-11-40-20.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300136852001_2023-10-21-09-41-22.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300136861001_2023-10-24-16-01-39.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300136907_2023-10-14-11-00-02.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300136928001_2023-11-07-10-46-40.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300136932_2023-11-03-15-17-18.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300136941001_2023-10-26-10-41-16.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300137144001_2023-10-21-10-43-07.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300137151001_2023-10-24-16-31-03.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300137163001_2023-10-25-16-51-47.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300137245001_2023-10-26-15-27-53.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300137272_2023-11-03-16-45-48.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300137325001_2023-10-23-16-21-54.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300137334001_2023-10-21-15-06-31.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300137388001_2023-10-26-11-26-40.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300137468001_2023-10-23-14-47-04.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300137669001_2023-11-07-14-48-44.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300137694001_2023-11-07-09-15-29.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300137710001_2023-11-04-14-52-37.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300137767001_2023-11-07-16-20-58.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300137871001_2023-11-06-15-10-33.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300137880001_2023-11-06-15-33-32.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300138007001_2023-11-07-15-58-20.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300138026001_2023-11-06-16-06-05.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300138047001_2023-11-04-16-34-30.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300138095001_2023-11-06-13-59-35.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_300138133001_2023-11-06-16-30-04.jpg', 16), ('./data/dataset\\\\ni_tai\\\\tongue_front_400110260001_2023-10-26-15-49-59.jpg', 16), ('./data/dataset\\\\pang_da\\\\tongue_front_300033407009_2023-10-21-09-19-22.jpg', 17), ('./data/dataset\\\\pang_da\\\\tongue_front_300128281002_2023-11-07-10-16-41.jpg', 17), ('./data/dataset\\\\pang_da\\\\tongue_front_300136486001_2023-10-26-10-30-40.jpg', 17), ('./data/dataset\\\\pang_da\\\\tongue_front_300136932_2023-11-03-15-17-18.jpg', 17), ('./data/dataset\\\\pang_da\\\\tongue_front_300137272_2023-11-03-16-45-48.jpg', 17), ('./data/dataset\\\\pang_da\\\\tongue_front_300137767001_2023-11-07-16-20-58.jpg', 17), ('./data/dataset\\\\qing_zi\\\\tongue_front_200011111001_2023-10-24-09-58-09.jpg', 18), ('./data/dataset\\\\qing_zi\\\\tongue_front_300061025010_2023-11-06-17-42-49.jpg', 18), ('./data/dataset\\\\qing_zi\\\\tongue_front_300064508006_2023-10-23-17-36-28.jpg', 18), ('./data/dataset\\\\qing_zi\\\\tongue_front_300098905003_2023-11-06-17-20-53.jpg', 18), ('./data/dataset\\\\qing_zi\\\\tongue_front_300116371003_2023-11-04-17-01-24.jpg', 18), ('./data/dataset\\\\qing_zi\\\\tongue_front_300124543006_2023-10-23-11-26-17.jpg', 18), ('./data/dataset\\\\qing_zi\\\\tongue_front_300126157003 _2023-11-07-11-07-19.jpg', 18), ('./data/dataset\\\\qing_zi\\\\tongue_front_300135824002_2023-10-23-11-03-41.jpg', 18), ('./data/dataset\\\\qing_zi\\\\tongue_front_300135893_2023-10-19-15-54-14.jpg', 18), ('./data/dataset\\\\qing_zi\\\\tongue_front_300136809001_2023-10-26-11-40-20.jpg', 18), ('./data/dataset\\\\qing_zi\\\\tongue_front_300137144001_2023-10-21-10-43-07.jpg', 18), ('./data/dataset\\\\qing_zi\\\\tongue_front_300137151001_2023-10-24-16-31-03.jpg', 18), ('./data/dataset\\\\qing_zi\\\\tongue_front_300137163001_2023-10-25-16-51-47.jpg', 18), ('./data/dataset\\\\qing_zi\\\\tongue_front_300137388001_2023-10-26-11-26-40.jpg', 18), ('./data/dataset\\\\qing_zi\\\\tongue_front_300137669001_2023-11-07-14-48-44.jpg', 18), ('./data/dataset\\\\qing_zi\\\\tongue_front_300138007001_2023-11-07-15-58-20.jpg', 18), ('./data/dataset\\\\shao_tai\\\\tongue_front_300021285011_2023-10-19-10-54-58.jpg', 19), ('./data/dataset\\\\she_jian_hong\\\\tongue_front_300125023004_2023-11-06-13-26-15.jpg', 20), ('./data/dataset\\\\she_jian_hong\\\\tongue_front_300130785002_2023-10-21-10-00-00.jpg', 20), ('./data/dataset\\\\she_jian_hong\\\\tongue_front_300137468001_2023-10-23-14-47-04.jpg', 20), ('./data/dataset\\\\she_jian_hong\\\\tongue_front_300138197001_2023-11-07-15-17-08.jpg', 20), ('./data/dataset\\\\she_jian_hong\\\\tongue_front_300138204_2023-11-07-15-36-45.jpg', 20), ('./data/dataset\\\\wai_xie\\\\tongue_front_197010022143_2023-10-14-10-39-30.jpg', 21), ('./data/dataset\\\\wai_xie\\\\tongue_front_200011111001_2023-10-24-09-58-09.jpg', 21), ('./data/dataset\\\\wai_xie\\\\tongue_front_300086582001_2023-10-21-15-39-18.jpg', 21), ('./data/dataset\\\\wai_xie\\\\tongue_front_300103974003_2023-10-21-10-26-48.jpg', 21), ('./data/dataset\\\\wai_xie\\\\tongue_front_300119198005_2023-10-26-15-07-02.jpg', 21), ('./data/dataset\\\\wai_xie\\\\tongue_front_300124543006_2023-10-23-11-26-17.jpg', 21), ('./data/dataset\\\\wai_xie\\\\tongue_front_300136928001_2023-11-07-10-46-40.jpg', 21), ('./data/dataset\\\\wai_xie\\\\tongue_front_300137245001_2023-10-26-15-27-53.jpg', 21), ('./data/dataset\\\\wei_ruan\\\\tongue_front_300128281002_2023-11-07-10-16-41.jpg', 22), ('./data/dataset\\\\wei_ruan\\\\tongue_front_300132464003_2023-10-20-09-55-35.jpg', 22), ('./data/dataset\\\\wei_ruan\\\\tongue_front_300137495001_2023-11-03-16-04-27.jpg', 22), ('./data/dataset\\\\wu_tai\\\\tongue_front_300131748002_2023-11-08-16-01-59.jpg', 23), ('./data/dataset\\\\wu_tai\\\\tongue_front_300136428001_2023-10-24-10-46-33.jpg', 23)]\n",
      "(<PIL.Image.Image image mode=RGB size=4000x3000 at 0x1EAF82A3ED0>, 0)\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "\n",
    "# 数据增强和预处理\n",
    "data_augmentation_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "])\n",
    "\n",
    "# 数据集根目录\n",
    "data_root = './data/dataset'\n",
    "\n",
    "dataset = ImageFolder(root=data_root, transform=data_augmentation_transform)\n",
    "print(len(dataset))\n",
    "print(dataset.classes)\n",
    "print(dataset.class_to_idx)\n",
    "print(dataset.imgs)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTImageProcessor {\n",
       "  \"_valid_processor_keys\": [\n",
       "    \"images\",\n",
       "    \"do_resize\",\n",
       "    \"size\",\n",
       "    \"resample\",\n",
       "    \"do_rescale\",\n",
       "    \"rescale_factor\",\n",
       "    \"do_normalize\",\n",
       "    \"image_mean\",\n",
       "    \"image_std\",\n",
       "    \"return_tensors\",\n",
       "    \"data_format\",\n",
       "    \"input_data_format\"\n",
       "  ],\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"image_processor_type\": \"ViTImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"resample\": 2,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 224,\n",
       "    \"width\": 224\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import ViTImageProcessor\n",
    "\n",
    "\n",
    "# 加载 Vision Transformer 的特征提取器\n",
    "model_name = 'google/vit-base-patch16-224-in21k'\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(model_name)\n",
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "\n",
    "# 计算每个类别的样本数量\n",
    "class_counts = torch.bincount(torch.tensor(dataset.targets))\n",
    "\n",
    "# 计算每个类别的权重\n",
    "class_weights = 1 / class_counts.float()\n",
    "\n",
    "# 设置随机数生成器种子\n",
    "seed = 42\n",
    "\n",
    "# 划分数据集\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "remaining_size = len(dataset) - train_size\n",
    "train_dataset, remaining_dataset = random_split(\n",
    "    dataset, [train_size, remaining_size])\n",
    "val_dataset, test_dataset = random_split(\n",
    "    remaining_dataset, [val_size, test_size])\n",
    "\n",
    "id2label = {id: label for id, label in enumerate(\n",
    "    train_dataset.dataset.classes)}\n",
    "label2id = {label: id for id,label in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "pixel_values torch.Size([4, 3, 224, 224])\n",
      "labels torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "\n",
    "def custom_collate(batch):\n",
    "\n",
    "\n",
    "\n",
    "    # 将样本数据列表拆分为输入和标签\n",
    "\n",
    "    images, labels = list(zip(*batch))\n",
    "\n",
    "    inputs = feature_extractor(images, return_tensors='pt')\n",
    "\n",
    "    inputs['labels'] = torch.tensor(labels)\n",
    "    inputs['pixel_values'] = torch.stack(\n",
    "        [input for input in inputs[\"pixel_values\"]])\n",
    "    return inputs\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, collate_fn=custom_collate, shuffle=True, drop_last=True)\n",
    "train_data = MyDataset(train_dataset)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, collate_fn=custom_collate, shuffle=False, drop_last=True)\n",
    "val_data=MyDataset(val_dataset)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, collate_fn=custom_collate, shuffle=False, drop_last=True)\n",
    "test_data=MyDataset(test_dataset)\n",
    "print(len(train_loader))\n",
    "batch = next(iter(train_loader))\n",
    "for k, v in batch.items():\n",
    "  if isinstance(v, torch.Tensor):\n",
    "    print(k, v.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(model_name, num_labels=len(dataset.classes),id2label=id2label,label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "metric_name = \"accuracy\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"tongue-disease-classification\",\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    logging_dir='logs',\n",
    "    remove_unused_columns=False,\n",
    "    seed=seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # predictions, labels = eval_pred\n",
    "    # predictions = np.argmax(predictions, axis=1)\n",
    "    # return dict(accuracy=accuracy_score(predictions, labels))\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argsort(axis=-1)[:, -5:]  # 获取top-5预测结果\n",
    "    # 将标签扩展为与预测结果相同的形状\n",
    "    labels_expanded = labels.reshape(-1, 1).repeat(5, axis=1)\n",
    "    correct_predictions = (predictions == labels_expanded)  # 判断预测结果是否包含正确标签\n",
    "    top5_accuracy = correct_predictions.any(axis=-1).mean()  # 计算top-5准确率\n",
    "    return {\"accuracy\": top5_accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ChenXi\\.conda\\envs\\2.2.1_11.8\\Lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    data_collator=custom_collate,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 20456), started 3:50:42 ago. (Use '!kill 20456' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start tensorboard.\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a93112361a04966a4dd71f1f1bf0ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae3fd6a2ee584a1fb4ddb72d36e43571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.7128612995147705, 'eval_accuracy': 0.6857142857142857, 'eval_runtime': 7.8931, 'eval_samples_per_second': 4.434, 'eval_steps_per_second': 0.633, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96dc7ba1061843948d80d0fb3d860fa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.7157540321350098, 'eval_accuracy': 0.6857142857142857, 'eval_runtime': 7.0109, 'eval_samples_per_second': 4.992, 'eval_steps_per_second': 0.713, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24fd9382cacf459f9b1cd205e9ebe6ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.730070114135742, 'eval_accuracy': 0.6285714285714286, 'eval_runtime': 6.9649, 'eval_samples_per_second': 5.025, 'eval_steps_per_second': 0.718, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b945c05198147f59dd3b3322793f916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.705512523651123, 'eval_accuracy': 0.6857142857142857, 'eval_runtime': 6.7098, 'eval_samples_per_second': 5.216, 'eval_steps_per_second': 0.745, 'epoch': 4.0}\n",
      "{'train_runtime': 265.6085, 'train_samples_per_second': 4.232, 'train_steps_per_second': 0.542, 'train_loss': 2.363711886935764, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=144, training_loss=2.363711886935764, metrics={'train_runtime': 265.6085, 'train_samples_per_second': 4.232, 'train_steps_per_second': 0.542, 'train_loss': 2.363711886935764, 'epoch': 4.0})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练 (废弃)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from transformers import ViTForImageClassification\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class VisionTransformer:\n",
    "    def __init__(self, model_name='google/vit-base-patch16-224-in21k', num_classes=24):\n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "        self.model = ViTForImageClassification.from_pretrained(model_name)\n",
    "        self.model.classifier = nn.Linear(\n",
    "            self.model.config.hidden_size, num_classes)\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def train(self, train_loader, val_loader, num_epochs=5, lr=1e-4, checkpoint_path='checkpoint.pth'):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        # 每 5 个 epoch 衰减学习率为当前的 0.1 倍\n",
    "        scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        best_val_loss = float('inf')\n",
    "        for epoch in range(num_epochs):\n",
    "            self.model.train()\n",
    "            running_loss = 0.0\n",
    "            for inputs in train_loader:\n",
    "                inputs = {key: value.to(self.device)\n",
    "                          for key, value in inputs.items()}\n",
    "                print(inputs)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(**inputs)\n",
    "                loss = criterion(outputs.logits, inputs['labels'])\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            scheduler.step()\n",
    "            epoch_loss = running_loss / len(train_loader.dataset)\n",
    "            train_losses.append(epoch_loss)\n",
    "\n",
    "            val_loss = self.evaluate(val_loader)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            print(\n",
    "                f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(self.model.state_dict(), checkpoint_path)\n",
    "\n",
    "        return train_losses, val_losses\n",
    "\n",
    "    def evaluate(self, data_loader):\n",
    "        self.model.eval()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        running_loss = 0.0\n",
    "        correct_top1 = 0\n",
    "        correct_top5 = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs in train_loader:\n",
    "                inputs = {key: value.to(self.device)\n",
    "                               for key, value in inputs.items()}\n",
    "\n",
    "                outputs = self.model(**inputs)\n",
    "                loss = criterion(outputs.logits, inputs['labels'])\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "\n",
    "                _, predicted = torch.topk(\n",
    "                    outputs.logits, k=5, dim=1)  # 获取前五个预测结果的索引\n",
    "                total += inputs['labels'].size(0)\n",
    "                # 计算 Top-1 和 Top-5 准确率\n",
    "                correct_top1 += (predicted[:, 0] ==\n",
    "                                 inputs['labels']).sum().item()\n",
    "                correct_top5 += torch.sum(torch.any(predicted == inputs['labels'].view(-1, 1),dim=1)).item()\n",
    "        accuracy_top1 = correct_top1 / total\n",
    "        accuracy_top5 = correct_top5 / total\n",
    "        print(f'Top1 Evaluate Accuracy: {accuracy_top1:.4f}')\n",
    "        print(f'Top5 Evaluate Accuracy: {accuracy_top5:.4f}')\n",
    "\n",
    "        return running_loss / len(data_loader.dataset)\n",
    "\n",
    "    def test(self, test_loader):\n",
    "        self.model.eval()\n",
    "        correct_top1 = 0\n",
    "        correct_top5 = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs in test_loader:\n",
    "                inputs = {key: value.to(self.device)\n",
    "                          for key, value in inputs.items()}\n",
    "\n",
    "                outputs = self.model(**inputs)\n",
    "                _, predicted = torch.topk(\n",
    "                    outputs.logits, k=5, dim=1)  # 获取前五个预测结果的索引\n",
    "                total += inputs['labels'].size(0)\n",
    "                # 计算 Top-1 和 Top-5 准确率\n",
    "                correct_top1 += (predicted[:, 0] ==\n",
    "                                 inputs['labels']).sum().item()\n",
    "                correct_top5 += torch.sum(torch.any(predicted ==\n",
    "                                          inputs['labels'].view(-1, 1), dim=1)).item()\n",
    "        accuracy_top1 = correct_top1 / total\n",
    "        accuracy_top5 = correct_top5 / total\n",
    "\n",
    "        print(f'Top1 Evaluate Accuracy: {accuracy_top1:.4f}')\n",
    "        print(f'Top5 Evaluate Accuracy: {accuracy_top5:.4f}')\n",
    "        return accuracy_top1, accuracy_top5\n",
    "\n",
    "    def plot_losses(self, train_losses, val_losses):\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(val_losses, label='Val Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def export_model(self, filename='vit_model.pt'):\n",
    "        self.model.eval().to('cpu')\n",
    "        torch.save(self.model, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初始化模型\n",
    "model = VisionTransformer()\n",
    "\n",
    "# 训练模型\n",
    "train_losses, val_losses = model.train(train_loader, val_loader, checkpoint_path='./model/checkpoint.pth')\n",
    "\n",
    "# 可视化训练过程\n",
    "model.plot_losses(train_losses, val_losses)\n",
    "\n",
    "# 加载最佳模型参数\n",
    "model.model.load_state_dict(torch.load('./model/checkpoint.pth'))\n",
    "\n",
    "# 测试模型\n",
    "model.test(test_loader)\n",
    "\n",
    "# 导出模型\n",
    "model.export_model('vit_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['labels'] for x in batch])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "labels = dataset.classes\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model,\n",
    "    num_labels=len(labels),\n",
    "    id2label={str(i): c for i, c in enumerate(labels)},\n",
    "    label2id={c: str(i) for i, c in enumerate(labels)}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./vit-base-tongue\",\n",
    "    per_device_train_batch_size=32,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=4,\n",
    "    fp16=True,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    report_to='tensorboard',\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from transformers import Trainer\n",
    "from accelerate import DataLoaderConfiguration\n",
    "\n",
    "\n",
    "# 定义数据加载器配置\n",
    "dataloader_config = DataLoaderConfiguration(\n",
    "    dispatch_batches=None,\n",
    "    split_batches=False,\n",
    "    even_batches=True,\n",
    "    use_seedable_sampler=True\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "\n",
    "    train_dataset=train_loader,\n",
    "\n",
    "    eval_dataset=val_loader,\n",
    "\n",
    "    tokenizer=feature_extractor,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_results = trainer.train()\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metrics = trainer.evaluate(prepared_ds['validation'])\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,  4,\n",
       "         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "         4,  4,  4,  4,  4,  4,  4,  4,  4,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
       "         5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "         6,  6,  6,  6,  6,  6,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,\n",
       "        10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
       "        11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 13, 13, 14, 14, 14, 14,\n",
       "        14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "        15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "        16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18,\n",
       "        18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 20, 20, 20, 20, 20, 21, 21, 21,\n",
       "        21, 21, 21, 21, 21, 22, 22, 22, 23, 23])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classes = os.listdir(data_root)  # 获取数据集类别\n",
    "# id2label={i: c for i, c in enumerate(classes)},\n",
    "# label2id={c: i for i, c in enumerate(classes)}\n",
    "\n",
    "# # 定义函数加载数据集并进行批量处理\n",
    "\n",
    "\n",
    "# def process_dataset(data_root, data_transform, feature_extractor):\n",
    "#     images = []\n",
    "#     labels = []\n",
    "\n",
    "#     # 遍历数据集文件夹\n",
    "#     for class_dir in os.listdir(data_root):\n",
    "#         class_path = os.path.join(data_root, class_dir)\n",
    "#         if not os.path.isdir(class_path):\n",
    "#             continue\n",
    "\n",
    "#         # 遍历每个类别文件夹\n",
    "#         for image_file in os.listdir(class_path):\n",
    "#             image_path = os.path.join(class_path, image_file)\n",
    "#             image = Image.open(image_path)\n",
    "#             image = data_transform(image)  # 数据预处理\n",
    "#             images.append(image)\n",
    "#             labels.append(label2id[class_dir])\n",
    "\n",
    "#     # 使用 ViTFeatureExtractor 进行批量处理\n",
    "#     inputs = feature_extractor(images=images, return_tensors=\"pt\")\n",
    "\n",
    "#     return inputs, torch.tensor(labels)\n",
    "\n",
    "\n",
    "# # 调用函数加载数据集并进行批量处理\n",
    "# inputs, labels = process_dataset(data_root, data_transform, feature_extractor)\n",
    "\n",
    "# # 输出处理后的张量\n",
    "# inputs\n",
    "# labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2.2.1_11.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
