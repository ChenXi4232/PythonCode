{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "['_background_', 'Tg']\n",
      "torch.Size([3, 32, 32])\n",
      "torch.Size([32, 32])\n",
      "tensor([0., 1.])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "\n",
    "class TongueDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_dir = os.path.join(root_dir, 'images')\n",
    "        self.mask_dir = os.path.join(root_dir, 'annotations')\n",
    "        self.class_names = self.load_class_names()\n",
    "\n",
    "    def load_class_names(self):\n",
    "        class_names_file = os.path.join(self.root_dir, 'class_names.txt')\n",
    "        with open(class_names_file, 'r') as f:\n",
    "            class_names = f.readlines()\n",
    "        class_names = [name.strip() for name in class_names]\n",
    "        return class_names\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.image_dir))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.listdir(self.image_dir)[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        mask_path = os.path.join(\n",
    "            self.mask_dir, os.path.splitext(img_name)[0] + '.png')\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        mask = Image.open(mask_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = transforms.Resize((32, 32))(mask)\n",
    "            mask = torch.tensor(np.array(mask), dtype=torch.float32)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = TongueDataset(root_dir='./data/split_dataset_ultra', transform=data_transform)\n",
    "\n",
    "print(len(dataset))\n",
    "print(dataset.class_names)\n",
    "print(dataset[0][0].shape)\n",
    "print(dataset[0][1].shape)\n",
    "print(dataset[0][1].unique())\n",
    "print(dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 32, 32]) torch.Size([2, 32, 32])\n",
      "tensor([0., 1.])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "aug_transform = transforms.Compose([\n",
    "    transforms.RandomApply(transforms.ColorJitter(brightness=0.2, contrast=0.2,\n",
    "                           saturation=0.2, hue=0.1)),\n",
    "    transforms.RandomApply(transforms.GaussianBlur(3, sigma=(0.1, 2.0)), 0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[\n",
    "                             0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[\n",
    "                             0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# 划分数据集\n",
    "train_size = int(0.6 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "train_dataloader.dataset.transform = aug_transform\n",
    "for i, (img, mask) in enumerate(train_dataloader):\n",
    "    print(img.shape, mask.shape)\n",
    "    print(mask.unique())\n",
    "    break\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "val_dataloader.dataset.transform = transform\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "test_dataloader.dataset.transform = transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "\n",
    "\n",
    "sys.setrecursionlimit(100000)  # 将默认的递归深度修改\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# 定义基于VGG16的FCN网络\n",
    "class VGG16_FCN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(VGG16_FCN, self).__init__()\n",
    "        # 加载预训练的VGG16模型\n",
    "        vgg16 = models.vgg16(\n",
    "            weights=models.VGG16_Weights.DEFAULT)\n",
    "\n",
    "        # 取出VGG16的前面部分（去掉全连接层）\n",
    "        self.features = vgg16.features\n",
    "\n",
    "        # 用1x1卷积替换VGG16的全连接层\n",
    "        self.conv6 = nn.Conv2d(512, 4096, kernel_size=5, padding=2)\n",
    "        self.relu6 = nn.ReLU(inplace=True)\n",
    "        self.dropout6 = nn.Dropout2d()\n",
    "        self.conv7 = nn.Conv2d(4096, 4096, kernel_size=3, padding=1)\n",
    "        self.relu7 = nn.ReLU(inplace=True)\n",
    "        self.dropout7 = nn.Dropout2d()\n",
    "\n",
    "        # 最后的卷积层用于生成分割结果\n",
    "        self.score_fr = nn.Conv2d(4096, num_classes, kernel_size=1)\n",
    "        self.upscore = nn.ConvTranspose2d(\n",
    "            num_classes, num_classes, kernel_size=64, stride=32, padding=16, bias=False)\n",
    "\n",
    "        # 初始化上采样层权重\n",
    "        self.upscore.weight.data.fill_(0)\n",
    "        self.upscore.weight.data[:, :, 16, 16] = 1  # 双线性插值\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 前向传播\n",
    "        x = self.features(x)\n",
    "        x = self.relu6(self.conv6(x))\n",
    "        x = self.dropout6(x)\n",
    "        x = self.relu7(self.conv7(x))\n",
    "        x = self.dropout7(x)\n",
    "        x = self.score_fr(x)\n",
    "        x = self.upscore(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "def convert_to_one_hot(labels):\n",
    "    # 获取原始标签中的类别数\n",
    "    num_classes = len(torch.unique(labels))\n",
    "\n",
    "    # 创建一个大小为 (B, C, W, H) 的零张量，其中 C 是类别数\n",
    "    one_hot_labels = torch.zeros(labels.size(\n",
    "        0), num_classes, *labels.size()[1:])\n",
    "\n",
    "    # 遍历每个样本的原始标签，并将相应位置的值设置为 1\n",
    "    for i in range(num_classes):\n",
    "        one_hot_labels[:, i, :, :] = (labels == i).float()\n",
    "\n",
    "    return one_hot_labels\n",
    "\n",
    "\n",
    "class SegmentationLoss(nn.Module):\n",
    "    def __init__(self, weight_bce=0.5, weight_connectivity=0, weight_smoothness=0):\n",
    "        super(SegmentationLoss, self).__init__()\n",
    "        self.weight_bce = weight_bce\n",
    "        self.weight_connectivity = weight_connectivity\n",
    "        self.weight_smoothness = weight_smoothness\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        self.smooth = 1e-6\n",
    "\n",
    "    def forward(self, logits, masks):\n",
    "        # 计算 BCE Loss\n",
    "        bce_loss = self.bce_loss(logits, masks)\n",
    "\n",
    "        # 计算 Dice Loss\n",
    "        probs = torch.sigmoid(logits)\n",
    "        intersection = torch.sum(probs * masks)\n",
    "        dice_loss = 1 - (2. * intersection + self.smooth) / \\\n",
    "            (torch.sum(probs) + torch.sum(masks) + self.smooth)\n",
    "        \n",
    "        if self.weight_connectivity != 0:\n",
    "            connectivity_loss = torch.tensor(0, dtype=torch.float32)\n",
    "            images = torch.argmax(probs, dim=1)\n",
    "            # masks = torch.argmax(masks, dim=1)\n",
    "            for i in range(images.size(0)):\n",
    "                zero_count_image, one_count_image, _, _ = self.connected_components(\n",
    "                    images[i])\n",
    "                # zero_count_target, one_count_target, _, _ = self.connected_components(\n",
    "                #     masks[i])\n",
    "                connectivity_loss += (zero_count_image - 1) ** 2 + \\\n",
    "                    (one_count_image - 1) ** 2\n",
    "            connectivity_loss /= images.size(0)\n",
    "        else:\n",
    "            connectivity_loss = torch.tensor(0, dtype=torch.float32)\n",
    "\n",
    "        if self.weight_smoothness != 0:\n",
    "            smoothness_loss = torch.tensor(0, dtype=torch.float32)\n",
    "            images = torch.argmax(probs, dim=1)\n",
    "            # masks = torch.argmax(masks, dim=1)\n",
    "            pi = torch.tensor(np.pi)\n",
    "            for i in range(images.size(0)):\n",
    "                _, _, _, area_perimeter1_image = self.connected_components(\n",
    "                    images[i])\n",
    "                # _, _, _, area_perimeter1_target = self.connected_components(\n",
    "                #     masks[i])\n",
    "                if area_perimeter1_image is None:\n",
    "                    smoothness_loss += 1\n",
    "                    continue\n",
    "                max_area_index_image = torch.argmax(area_perimeter1_image[:, 0])\n",
    "                # max_area_index_target = torch.argmax(area_perimeter1_target[:, 0])\n",
    "                perimeter_area_ratio_image = area_perimeter1_image[max_area_index_image][1] ** 2 * pi / \\\n",
    "                    (4 * area_perimeter1_image[max_area_index_image][0])\n",
    "                # perimeter_area_ratio_target = area_perimeter1_target[max_area_index_target][1] ** 2 * pi / \\\n",
    "                #     (4 * area_perimeter1_target[max_area_index_target][0])\n",
    "                smoothness_loss += (perimeter_area_ratio_image - 1) ** 2\n",
    "            smoothness_loss /= images.size(0)\n",
    "        else:\n",
    "            smoothness_loss = torch.tensor(0, dtype=torch.float32)\n",
    "\n",
    "        # 加权结合损失\n",
    "        combined_loss = self.weight_bce * bce_loss + \\\n",
    "            (1 - self.weight_bce) * dice_loss + \\\n",
    "            self.weight_connectivity * connectivity_loss + \\\n",
    "            self.weight_smoothness * smoothness_loss\n",
    "\n",
    "        return combined_loss\n",
    "    \n",
    "    def flood_fill(self, image, x, y, target_color, visited, area_perimeter):\n",
    "        visited.add((x, y))\n",
    "        area_perimeter[0][0] += 1\n",
    "        area_perimeter[0][1] += 4\n",
    "\n",
    "        if x > 0 and image[x - 1][y] == target_color:\n",
    "            area_perimeter[0][1] -= 1\n",
    "            if (x - 1, y) not in visited:\n",
    "                self.flood_fill(image, x - 1, y, target_color,\n",
    "                                visited, area_perimeter)\n",
    "        if x < image.size(0) - 1 and image[x + 1][y] == target_color:\n",
    "            area_perimeter[0][1] -= 1\n",
    "            if (x + 1, y) not in visited:\n",
    "                self.flood_fill(image, x + 1, y, target_color,\n",
    "                                visited, area_perimeter)\n",
    "        if y > 0 and image[x][y - 1] == target_color:\n",
    "            area_perimeter[0][1] -= 1\n",
    "            if (x, y - 1) not in visited:\n",
    "                self.flood_fill(image, x, y - 1, target_color,\n",
    "                                visited, area_perimeter)\n",
    "        if y < image.size(1) - 1 and image[x][y + 1] == target_color:\n",
    "            area_perimeter[0][1] -= 1\n",
    "            if (x, y + 1) not in visited:\n",
    "                self.flood_fill(image, x, y + 1, target_color,\n",
    "                                visited, area_perimeter)\n",
    "\n",
    "    def connected_components(self, image):\n",
    "        visited = set()\n",
    "        zero_count = torch.tensor(0, dtype=torch.float32)\n",
    "        one_count = torch.tensor(0, dtype=torch.float32)\n",
    "        area_perimeter0 = None\n",
    "        area_perimeter1 = None\n",
    "\n",
    "        for i in range(image.size(0)):\n",
    "            for j in range(image.size(1)):\n",
    "                if (i, j) not in visited:\n",
    "                    if image[i][j] == 0:\n",
    "                        zero_count += 1\n",
    "                        temp = torch.zeros((1, 2), dtype=torch.float32)\n",
    "                        self.flood_fill(image, i, j, 0, visited, temp)\n",
    "                        if area_perimeter0 is None:\n",
    "                            area_perimeter0 = temp.clone()\n",
    "                        else:\n",
    "                            area_perimeter0 = torch.cat(\n",
    "                                (area_perimeter0, temp), dim=0)  # 添加到张量\n",
    "                    elif image[i][j] == 1:\n",
    "                        one_count += 1\n",
    "                        temp = torch.zeros((1, 2), dtype=torch.float32)\n",
    "                        self.flood_fill(image, i, j, 1, visited, temp)\n",
    "                        if area_perimeter1 is None:\n",
    "                            area_perimeter1 = temp.clone()\n",
    "                        else:\n",
    "                            area_perimeter1 = torch.cat(\n",
    "                                (area_perimeter1, temp), dim=0)  # 添加到张量\n",
    "\n",
    "        # 跳过初始全零项\n",
    "        return zero_count, one_count, area_perimeter0, area_perimeter1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, num_epochs=15):\n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练模型\n",
    "        model.train()\n",
    "        train_running_loss = 0.0\n",
    "        for i in range(3):\n",
    "            if i == 0:\n",
    "                criterion = SegmentationLoss()\n",
    "            elif i == 1:\n",
    "                criterion = SegmentationLoss(weight_connectivity=0.5)\n",
    "            else:\n",
    "                criterion = SegmentationLoss(weight_smoothness=0.5)\n",
    "            for images, labels in tqdm(train_loader):\n",
    "                images = images.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                # print(outputs.size())\n",
    "                outputs_resized = F.interpolate(outputs, size=(images.size(\n",
    "                    2), images.size(3)), mode='bilinear', align_corners=True)\n",
    "                labels_for_loss = convert_to_one_hot(labels).to(device)\n",
    "                loss = criterion(outputs_resized, labels_for_loss)  # 根据需要定义损失函数\n",
    "                # loss = criterion(outputs, labels_for_loss)\n",
    "                loss.backward()\n",
    "                # if i == 2:\n",
    "                train_running_loss += loss.item()\n",
    "                optimizer.step()\n",
    "    \n",
    "            # 打印每个epoch的损失\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_running_loss/len(train_loader)}\")\n",
    "            torch.save(model.state_dict(), f'vgg16_fcn_optim_{epoch+1}_{i+1}.pth')\n",
    "\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        IOU = 0\n",
    "        iou_class1 = 0\n",
    "        iou_class2 = 0\n",
    "        criterion = SegmentationLoss()\n",
    "        for images, labels in tqdm(val_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(images)\n",
    "            outputs_resized = F.interpolate(outputs, size=(images.size(\n",
    "                2), images.size(3)), mode='bilinear', align_corners=True)\n",
    "            one_hot_labels = convert_to_one_hot(labels).to(device)\n",
    "            loss = criterion(outputs_resized, one_hot_labels)\n",
    "            # loss = criterion(outputs, labels_for_loss)\n",
    "            val_running_loss += loss.item()\n",
    "            outputs = torch.sigmoid(outputs_resized)  # 使用sigmoid函数将输出限制在0到1之间\n",
    "\n",
    "            # 分别取出两个类别的输出和标签\n",
    "            outputs_class1 = outputs[:, 0, :, :]  # 第一个类别的输出\n",
    "            outputs_class2 = outputs[:, 1, :, :]  # 第二个类别的输出\n",
    "            labels_class1 = one_hot_labels[:, 0, :, :]    # 第一个类别的标签\n",
    "            labels_class2 = one_hot_labels[:, 1, :, :]    # 第二个类别的标签\n",
    "\n",
    "            # 计算第一个类别的交并比（IOU）\n",
    "            intersection_class1 = torch.sum(outputs_class1 * labels_class1)\n",
    "            union_class1 = torch.sum(outputs_class1) + torch.sum(labels_class1)\n",
    "            temp_iou1 = (intersection_class1 + 1e-6) / (union_class1 - intersection_class1 + 1e-6)\n",
    "            iou_class1 += temp_iou1\n",
    "\n",
    "            # 计算第二个类别的交并比（IOU）\n",
    "            intersection_class2 = torch.sum(outputs_class2 * labels_class2)\n",
    "            union_class2 = torch.sum(outputs_class2) + torch.sum(labels_class2)\n",
    "            temp_iou2 = (intersection_class2 + 1e-6) / (union_class2 - intersection_class2 + 1e-6)\n",
    "            iou_class2 += temp_iou2\n",
    "\n",
    "            # 计算平均IOU\n",
    "            IOU += (temp_iou1 + temp_iou2) / 2\n",
    "\n",
    "            # 输出遮罩\n",
    "            # outputs = torch.argmax(outputs, dim=1)\n",
    "            # mask1 = outputs[0].squeeze().cpu().numpy()\n",
    "            # final_mask1 = np.zeros_like(mask1)\n",
    "            # final_mask1[mask1 == 1] = 255\n",
    "            # final_mask1 = Image.fromarray(final_mask1.astype(np.uint8))\n",
    "            # final_mask1.save('mask1.png')\n",
    "            # label_mask1 = labels[0].squeeze().cpu().numpy()\n",
    "            # final_label_mask1 = np.zeros_like(label_mask1)\n",
    "            # final_label_mask1[label_mask1 == 1] = 255\n",
    "            # final_label_mask1 = Image.fromarray(final_label_mask1.astype(np.uint8))\n",
    "            # final_label_mask1.save('label_mask1.png')\n",
    "            # mask2 = outputs[1].squeeze().cpu().numpy()\n",
    "            # final_mask2 = np.zeros_like(mask2)\n",
    "            # final_mask2[mask2 == 1] = 255\n",
    "            # final_mask2 = Image.fromarray(final_mask2.astype(np.uint8))\n",
    "            # final_mask2.save('mask2.png')\n",
    "            # label_mask2 = labels[1].squeeze().cpu().numpy()\n",
    "            # final_label_mask2 = np.zeros_like(label_mask2)\n",
    "            # final_label_mask2[label_mask2 == 1] = 255\n",
    "            # final_label_mask2 = Image.fromarray(final_label_mask2.astype(np.uint8))\n",
    "            # final_label_mask2.save('label_mask2.png')\n",
    "\n",
    "        \n",
    "        print(\n",
    "            f\"Validation Loss: {val_running_loss/len(val_loader)}, mIOU: {IOU/len(val_loader)}, IOU Background: {iou_class1/len(val_loader)}, IOU Tongue: {iou_class2/len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:30<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Loss: 0.34826369126637774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [05:14<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Loss: 0.6576357787847519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [07:33<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Loss: 125.0053027099371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:29<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.28452641382813454, mIOU: 0.5641860961914062, IOU Background: 0.752784788608551, IOU Tongue: 0.37558725476264954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [03:25<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15, Loss: 0.30176383929948014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [07:53<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15, Loss: 0.615066617478927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [06:16<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15, Loss: 124.79949548100431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:48<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2845044917613268, mIOU: 0.5636608600616455, IOU Background: 0.7528432011604309, IOU Tongue: 0.37447860836982727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:33<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15, Loss: 0.2981033211946487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [06:49<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15, Loss: 0.6000181845078866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [07:38<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15, Loss: 130.6844944265733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:13<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2813735644519329, mIOU: 0.5690014362335205, IOU Background: 0.7586777806282043, IOU Tongue: 0.37932533025741577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [03:27<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15, Loss: 0.2975315910826127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [07:37<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15, Loss: 0.5949972349653642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [06:01<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15, Loss: 123.205424633647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:49<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2830453377217054, mIOU: 0.5667007565498352, IOU Background: 0.7543577551841736, IOU Tongue: 0.3790437877178192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:24<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15, Loss: 0.2974910165121158\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 33\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# 打印每个epoch的损失\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_running_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvgg16_fcn_optim_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     35\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     36\u001b[0m val_running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "num_classes = 2  # Background and tongue\n",
    "model = VGG16_FCN(num_classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_dataloader, val_dataloader, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# 定义基于VGG16的FCN网络\n",
    "class VGG16_FCN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(VGG16_FCN, self).__init__()\n",
    "        # 加载预训练的VGG16模型\n",
    "        vgg16 = models.vgg16(\n",
    "            weights=models.VGG16_Weights.DEFAULT)\n",
    "\n",
    "        # 取出VGG16的前面部分（去掉全连接层）\n",
    "        self.features = vgg16.features\n",
    "\n",
    "        # 用1x1卷积替换VGG16的全连接层\n",
    "        self.conv6 = nn.Conv2d(512, 4096, kernel_size=1)\n",
    "        self.relu6 = nn.ReLU(inplace=True)\n",
    "        self.dropout6 = nn.Dropout2d()\n",
    "        self.conv7 = nn.Conv2d(4096, 4096, kernel_size=1)\n",
    "        self.relu7 = nn.ReLU(inplace=True)\n",
    "        self.dropout7 = nn.Dropout2d()\n",
    "\n",
    "        # 最后的卷积层用于生成分割结果\n",
    "        self.score_fr = nn.Conv2d(4096, num_classes, kernel_size=1)\n",
    "        self.upscore = nn.ConvTranspose2d(\n",
    "            num_classes, num_classes, kernel_size=64, stride=32, padding=16, bias=False)\n",
    "\n",
    "        # 初始化上采样层权重\n",
    "        self.upscore.weight.data.fill_(0)\n",
    "        self.upscore.weight.data[:, :, 16, 16] = 1  # 双线性插值\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 前向传播\n",
    "        x = self.features(x)\n",
    "        x = self.relu6(self.conv6(x))\n",
    "        x = self.dropout6(x)\n",
    "        x = self.relu7(self.conv7(x))\n",
    "        x = self.dropout7(x)\n",
    "        x = self.score_fr(x)\n",
    "        x = self.upscore(x)\n",
    "        return x  \n",
    "\n",
    "\n",
    "def get_segmentation_mask(model, image_path):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[\n",
    "                             0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    image = transform(image).unsqueeze(0)\n",
    "    print(image.size())\n",
    "    image = image.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "    output = torch.sigmoid(output)\n",
    "    output = torch.argmax(output, dim=1)\n",
    "    mask = output.squeeze().cpu().numpy()\n",
    "    return mask\n",
    "\n",
    "# Get segmentation mask for a sample image\n",
    "\n",
    "image_path = './data/test/label_data/test1.jpg'  # Replace with your image path\n",
    "# 载入模型权重\n",
    "model = VGG16_FCN(num_classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "mask = get_segmentation_mask(model, image_path)\n",
    "\n",
    "\n",
    "# Create the final mask where tongue region is white and background is black\n",
    "\n",
    "final_mask = np.zeros_like(mask)\n",
    "\n",
    "final_mask[mask == 0] = 255\n",
    "final_mask = Image.fromarray(final_mask.astype(np.uint8))\n",
    "\n",
    "# 叠加原图\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "image = image.resize(final_mask.size)\n",
    "image.paste(final_mask, (0, 0), final_mask)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 生成随机的形状为 (2, 2, 3, 3) 的张量，值在 0 到 1 之间\n",
    "random_tensor = np.random.rand(2, 2, 3, 3)\n",
    "\n",
    "print(random_tensor)\n",
    "\n",
    "# 将 NumPy 数组转换为 PyTorch 张量\n",
    "random_tensor = torch.tensor(random_tensor, dtype=torch.float32)\n",
    "print(torch.sigmoid(random_tensor))\n",
    "print(torch.softmax(random_tensor, dim=1))\n",
    "print(torch.argmax(random_tensor, dim=1, keepdim=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2.2.1_11.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
