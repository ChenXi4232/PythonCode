{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(Module):\n",
    "    def _init_(self, in_features, out_features, bias=True):\n",
    "        super(Graphconvolution, self)._init_()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "            self.reset_parameters()\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. /. math.sqrt(self.weight.size(1))self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj. support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            réturn output\n",
    "        def  _repr_(self):\n",
    "            return self._class_._name_  + \"(\" + str(self.in_features) + str(self.out_features)+ \")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n",
      "torch.Size([2708]) torch.Size([2708]) torch.Size([2708])\n",
      "7 torch.Size([2708, 1433]) torch.Size([2708])\n",
      "SparseMatrix(indices=tensor([[   0,    0,    0,  ..., 2707, 2707, 2707],\n",
      "                             [ 633, 1862, 2582,  ...,  598, 1473, 2706]]),\n",
      "             values=tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
      "             shape=(2708, 2708), nnz=10556)\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "from dgl.data import citation_graph as citegrh\n",
    "\n",
    "\n",
    "# 下载并加载 Cora 数据集\n",
    "dataset_cora = citegrh.load_cora(raw_dir=\"./dataset/cora/\")\n",
    "\n",
    "g_cora = dataset_cora[0]\n",
    "num_class_cora = dataset_cora.num_classes\n",
    "feature_cora = g_cora.ndata['feat']\n",
    "label_cora = g_cora.ndata['label']\n",
    "\n",
    "# get data split\n",
    "train_mask_cora = g_cora.ndata['train_mask']\n",
    "val_mask_cora = g_cora.ndata['val_mask']\n",
    "test_mask_cora = g_cora.ndata['test_mask']\n",
    "\n",
    "print(train_mask_cora.size(), val_mask_cora.size(), test_mask_cora.size())\n",
    "print(num_class_cora, feature_cora.size(), label_cora.size())\n",
    "print(g_cora.adjacency_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ./dataset/citeseer/citeseer.zip from https://data.dgl.ai/dataset/citeseer.zip...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./dataset/citeseer/citeseer.zip: 100%|██████████| 239k/239k [00:00<00:00, 815kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting file to ./dataset/citeseer/citeseer_d6836239\n",
      "Finished data loading and preprocessing.\n",
      "  NumNodes: 3327\n",
      "  NumEdges: 9228\n",
      "  NumFeats: 3703\n",
      "  NumClasses: 6\n",
      "  NumTrainingSamples: 120\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done saving data into cached files.\n",
      "torch.Size([3327]) torch.Size([3327]) torch.Size([3327])\n",
      "6 torch.Size([3327, 3703]) torch.Size([3327])\n"
     ]
    }
   ],
   "source": [
    "# 下载并加载 Citeseer 数据集\n",
    "dataset_citeseer = citegrh.load_citeseer(raw_dir=\"./dataset/citeseer/\")\n",
    "\n",
    "g_citeseer = dataset_citeseer[0]\n",
    "num_class_citeseer = dataset_citeseer.num_classes\n",
    "feature_citeseer = g_citeseer.ndata['feat']\n",
    "label_citeseer = g_citeseer.ndata['label']\n",
    "\n",
    "# get data split\n",
    "train_mask_citeseer = g_citeseer.ndata['train_mask']\n",
    "val_mask_citeseer = g_citeseer.ndata['val_mask']\n",
    "test_mask_citeseer = g_citeseer.ndata['test_mask']\n",
    "\n",
    "print(train_mask_citeseer.size(), val_mask_citeseer.size(), test_mask_citeseer.size())\n",
    "print(num_class_citeseer, feature_citeseer.size(), label_citeseer.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import function as fn\n",
    "\n",
    "\n",
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        nn.init.xavier_uniform_(self.weight)  # 初始化权重\n",
    "        nn.init.zeros_(self.bias)  # 初始化偏置\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = features\n",
    "            g.update_all(message_func=fn.copy_u('h', 'm'),\n",
    "                        reduce_func=fn.sum('m', 'h'))\n",
    "            h = g.ndata['h']\n",
    "            h = torch.matmul(h, self.weight) + self.bias\n",
    "            return h\n",
    "    \n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gc1 = GraphConvolution(in_features, hidden_features)\n",
    "        self.gc2 = GraphConvolution(hidden_features, out_features)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        h = F.relu(self.gc1(g, features))\n",
    "        h = self.gc2(g, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import optuna\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # 定义一个目标函数，用于在 Optuna 中进行优化\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)  # 在对数尺度上搜索学习率\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(g, features)\n",
    "        loss = criterion(output[train_mask], labels[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 在验证集上计算损失\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_output = model(g, features)\n",
    "            val_loss = criterion(val_output[val_mask], labels[val_mask])\n",
    "\n",
    "        trial.report(val_loss.item(), epoch)  # 将验证集损失报告给 Optuna\n",
    "        if trial.should_prune():  # 如果性能不佳，剪枝（提前停止）\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    return val_loss.item()\n",
    "\n",
    "\n",
    "def train_model(model, g, features, labels, train_mask, val_mask, epochs=100):\n",
    "    study = optuna.create_study(direction='minimize')  # 创建一个 Optuna study 实例\n",
    "    study.optimize(objective, n_trials=100)  # 运行优化器，尝试不同的超参数组合\n",
    "\n",
    "    print('最佳学习率:', study.best_params['lr'])  # 打印最佳学习率\n",
    "\n",
    "    # 使用最佳学习率重新训练模型\n",
    "    lr = study.best_params['lr']\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(g, features)\n",
    "        loss = criterion(output[train_mask], labels[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 在验证集上计算损失\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_output = model(g, features)\n",
    "            val_loss = criterion(val_output[val_mask], labels[val_mask])\n",
    "\n",
    "        print(\n",
    "            f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "\n",
    "    print('训练完成。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 划分数据集\n",
    "\n",
    "\n",
    "def split_dataset(num_nodes, train_ratio=0.1, val_ratio=0.1, test_ratio=0.8):\n",
    "    indices = np.arange(num_nodes)\n",
    "    np.random.shuffle(indices)\n",
    "    train_size = int(num_nodes * train_ratio)\n",
    "    val_size = int(num_nodes * val_ratio)\n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:train_size+val_size]\n",
    "    test_indices = indices[train_size+val_size:]\n",
    "    train_mask = np.zeros(num_nodes, dtype=bool)\n",
    "    val_mask = np.zeros(num_nodes, dtype=bool)\n",
    "    test_mask = np.zeros(num_nodes, dtype=bool)\n",
    "    train_mask[train_indices] = True\n",
    "    val_mask[val_indices] = True\n",
    "    test_mask[test_indices] = True\n",
    "    return train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "# 划分 Cora 数据集\n",
    "num_nodes = g.num_nodes()\n",
    "train_mask, val_mask, test_mask = split_dataset(num_nodes)\n",
    "\n",
    "print(\n",
    "    f'Train nodes: {train_mask.sum()}, Val nodes: {val_mask.sum()}, Test nodes: {test_mask.sum()}')\n",
    "\n",
    "# 训练模型\n",
    "model = GCN(in_features=features.shape[1],\n",
    "            hidden_features=16, out_features=labels.max().item() + 1)\n",
    "train_model(model, g, features, labels, train_mask, val_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分数据集\n",
    "def split_edges(g, val_ratio=0.1, test_ratio=0.1):\n",
    "    num_edges = g.num_edges()\n",
    "    edges = np.arange(num_edges)\n",
    "    np.random.shuffle(edges)\n",
    "    val_size = int(num_edges * val_ratio)\n",
    "    test_size = int(num_edges * test_ratio)\n",
    "    val_edges = edges[:val_size]\n",
    "    test_edges = edges[val_size:val_size+test_size]\n",
    "    train_edges = edges[val_size+test_size:]\n",
    "    return train_edges, val_edges, test_edges\n",
    "\n",
    "\n",
    "# 划分 Cora 数据集的边\n",
    "train_edges, val_edges, test_edges = split_edges(g)\n",
    "\n",
    "print(\n",
    "    f'Train edges: {len(train_edges)}, Val edges: {len(val_edges)}, Test edges: {len(test_edges)}')\n",
    "\n",
    "# 训练模型\n",
    "model = GCN(in_features=features.shape[1], hidden_features=16, out_features=1)\n",
    "train_model(model, g, features, labels, train_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择最佳超参数组合\n",
    "best_hidden_size = 16\n",
    "best_lr = 0.01\n",
    "\n",
    "# 重新训练模型\n",
    "model = GCN(in_features=features.shape[1],\n",
    "            hidden_features=best_hidden_size, out_features=labels.max().item() + 1)\n",
    "train_model(model, g, features, labels, train_mask, val_mask, lr=best_lr)\n",
    "\n",
    "# 在测试集上进行最终测试\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_output = model(g, features)\n",
    "    test_loss = criterion(test_output[test_mask], labels[test_mask])\n",
    "\n",
    "print(f'Final Test Loss: {test_loss.item():.4f}')\n",
    "\n",
    "# 计算 Top 1 Accuracy\n",
    "_, predicted = torch.max(test_output[test_mask], 1)\n",
    "correct = (predicted == labels[test_mask]).sum().item()\n",
    "total = test_mask.sum().item()\n",
    "accuracy = correct / total\n",
    "print(f'Top 1 Accuracy: {accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2.2.1_11.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
